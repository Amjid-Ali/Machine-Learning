{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple linear regression\n",
    "\n",
    "Multiple linear regression (MLR/multiple regression) is a statistical technique. It can use several variables to predict the outcome of a different variable. The goal of multiple regression is to model the linear relationship between your independent variables and your dependent variable. It looks at how multiple independent variables are related to a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "these assumptions are also for linear Regression\n",
    "\n",
    "There are some assumptions that absolutely have to be true:\n",
    "\n",
    "There is a linear relationship between the dependent variable and the independent variables.\n",
    "\n",
    "The independent variables aren’t too highly correlated with each other.\n",
    "\n",
    "Your observations for the dependent variable are selected independently and at random.\n",
    "\n",
    "Regression residuals are normally distributed.\n",
    "\n",
    "1)linearity \n",
    "2)homoscedasticity\n",
    "3)multivariate normality\n",
    "4)independence of errors\n",
    "5)lack of multicollinearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dummy variables\n",
    "dummy variables are the new generated variables for categorical data,so these are called dummy variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dummy variable Trap\n",
    "when one or serval indepented variables in linear regression predict another is called multicollinearity\n",
    "\n",
    "Beware the dummy variable trap\n",
    "You never want to include both variables at the same time.\n",
    "Why is that?\n",
    "You’d be duplicating a variable. The first variable (d1) is always equal to 1 minus the second variable (d2). (d1 = 1-d2) When one variable predicts another, it’s called multicollinearity. As a result, the model wouldn’t be able to distinguish the results of d1 from the results of d2. You can’t have the constant and both dummy variables at the same time. If you have nine variables, include eight of them. (If you have two sets of dummy variables, then you have to do this for each set.)\n",
    "\n",
    "the rules for adding dummy variables in model will be dV-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is P value\n",
    "What is the P-value?\n",
    "You’re going to want to be familiar with the concept of a P-value. That’s definitely going to come up.\n",
    "The P-value is the probability of getting a sample like ours (or more extreme than ours) if the null hypothesis is true.\n",
    "It gives a value to the weirdness of your sample. If you have a large P-value, then you probably won’t change your mind about the null hypothesis. A large value means that it wouldn’t be at all surprising to get a sample like yours if the hypothesis is true. As the P-value gets smaller, you should probably start to ask yourself some questions. You might want to change your mind and maybe even reject the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are essentially five methods of building a multiple linear regression model.\n",
    " 1)Chuck Everything In and Hope for the Best\n",
    " 2)Backward Elimination\n",
    " 3)Forward Selection\n",
    " 4)Bidirectional Elimination\n",
    " 5)Score Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Chuck Everything In):\n",
    " Okay. That isn’t the official name for this method (but it should be). Occasionally you’ll need to build a model where you just throw in all your variables. You might have some kind of prior knowledge. You might have a particular framework you need to use. You might have been hired by someone who’s insisting that you do that. You might want to prepare for backward elimination. It’s a real option, so I’m including it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method 2 (backward elimination):\n",
    "This has a few basic steps.\n",
    "First, you’ll need to set a significance level for which data will stay in the model. For example, you might want to set a significance level of 5% (SL = 0.05). This is important and can have real ramifications, so give it some thought.\n",
    "(lower the level than SL will be in model)\n",
    "(p>sl),then fit,finilize it\n",
    "\n",
    "Next, you’ll fit the full model with all possible predictors.\n",
    "You’ll consider the predictor with the highest P-value. If your P-value is greater than your significance level, you’ll move to step four, otherwise, you’re done!\n",
    "Remove that predictor with the highest P-value\n",
    "\n",
    "Fit the model without that predictor variable. If you just remove the variable, you need to refit and rebuild the model. The coefficients and constants will be different. When you remove one, it affects the others.\n",
    "\n",
    "Go back to step 3, do it all over, and keep doing that until you come to a point where even the highest P-value is < SL. Now \n",
    "\n",
    "your model is ready. All of the variables that are left are less than the significance level.\n",
    "\n",
    "(After we go through these concepts, I’ll walk you through an example of backward elimination so you can see it in action! It’s definitely confusing, but if you really look at what’s going on, you’ll get the hang of it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3 (forward selection): This is way more complex than just reversing backward elimination.\n",
    "Choose your significance level (SL = 0.05).\n",
    "Fit all possible simple regression models and select the one with the lowest P-value.\n",
    "\n",
    "Keep this variable and fit all possible models with one extra predictor added to the one you already have. If we selected a simple linear regressor with one variable, now we’d select all of them with two variables. That means all possible two variable linear regressions.\n",
    "\n",
    "Find the predictor with the lowest P-value. If P < Sl, go back to step 3. Otherwise, you’re done!\n",
    "\n",
    "We can stop when P<SL is no longer true, or there are no more P-values that are less than the significance level. It means that \n",
    "\n",
    "the variable is not significant anymore. You won’t keep the current model, though. You’ll keep the previous one because, in the \n",
    "\n",
    "final model, your variable is insignificant.\n",
    "\n",
    "we keep the perivous model not the current model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4 (bidirectional elimination): This method combines the previous two!\n",
    "# this is also called stepwise regression\n",
    "Select a significance level to enter and a significance level to stay (SLENTER = 0.05, SLSTAY = 0.05).\n",
    "slenter is foward selection and slstay is for back elimnation\n",
    "\n",
    "Perform the next step of forward selection where you add the new variable. You need to have your P-value be less than SLENTER.\n",
    "\n",
    "Now perform all of the steps of backward elimination. The variables must have a P-value less than SLSTAY in order to stay.\n",
    "\n",
    "Now head back to step two, then move forward to step 3, and so on until no new variables can enter and no new variables can \n",
    "\n",
    "exit.\n",
    "You’re done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 5 (score comparison): Here, you’re going to be looking at all possible methods. You’ll look at a comparison of the scores for all of the possible methods. This is definitely the most resource-consuming approach!\n",
    "Select a criterion of goodness of fit (for example, Akaike criterion)\n",
    "Construct all possible regression models\n",
    "Select the one with the best criterion\n",
    "Fun fact: if you have 10 columns of data, you’ll wind up with 1,023 models here. You’d better be ready to commit if you’re going to go this route!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset loading \n",
    "dataset=pd.read_csv(\"./50_Startups.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engnering\n",
    "features=dataset.iloc[:,:-1].values\n",
    "label=dataset.iloc[:,4].values\n",
    "# foward_selection=data.iloc[:,:-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features=pd.DataFrame(features)\n",
    "# label=pd.DataFrame(label)\n",
    "# features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer\n",
    "labelEncoder_x=LabelEncoder()\n",
    "features[:,3]=labelEncoder_x.fit_transform(features[:,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>192261.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>191792.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>191050.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>182901.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>166187.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131876.90</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "      <td>156991.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "      <td>156122.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "      <td>155752.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "      <td>152211.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "      <td>149759.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>146121.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>144259.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "      <td>141585.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "      <td>134307.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "      <td>132602.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "      <td>129917.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "      <td>126992.93</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "      <td>125370.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "      <td>124266.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86419.70</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>122776.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.30</td>\n",
       "      <td>298664.47</td>\n",
       "      <td>118474.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "      <td>111313.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>110352.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "      <td>108733.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "      <td>108552.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "      <td>107404.34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "      <td>105733.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72107.60</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>105008.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.20</td>\n",
       "      <td>103282.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "      <td>101004.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "      <td>99937.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>97483.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "      <td>97427.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "      <td>96778.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "      <td>96712.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "      <td>96479.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "      <td>90708.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "      <td>89949.14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.10</td>\n",
       "      <td>81229.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.30</td>\n",
       "      <td>81005.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "      <td>78239.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>77798.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "      <td>71498.49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.30</td>\n",
       "      <td>35534.17</td>\n",
       "      <td>69758.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "      <td>65200.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "      <td>64926.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "      <td>49490.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42559.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35673.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00</td>\n",
       "      <td>116983.80</td>\n",
       "      <td>45173.06</td>\n",
       "      <td>14681.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3    4    5    6\n",
       "0   165349.20  136897.80  471784.10  192261.83  0.0  0.0  1.0\n",
       "1   162597.70  151377.59  443898.53  191792.06  1.0  0.0  0.0\n",
       "2   153441.51  101145.55  407934.54  191050.39  0.0  1.0  0.0\n",
       "3   144372.41  118671.85  383199.62  182901.99  0.0  0.0  1.0\n",
       "4   142107.34   91391.77  366168.42  166187.94  0.0  1.0  0.0\n",
       "5   131876.90   99814.71  362861.36  156991.12  0.0  0.0  1.0\n",
       "6   134615.46  147198.87  127716.82  156122.51  1.0  0.0  0.0\n",
       "7   130298.13  145530.06  323876.68  155752.60  0.0  1.0  0.0\n",
       "8   120542.52  148718.95  311613.29  152211.77  0.0  0.0  1.0\n",
       "9   123334.88  108679.17  304981.62  149759.96  1.0  0.0  0.0\n",
       "10  101913.08  110594.11  229160.95  146121.95  0.0  1.0  0.0\n",
       "11  100671.96   91790.61  249744.55  144259.40  1.0  0.0  0.0\n",
       "12   93863.75  127320.38  249839.44  141585.52  0.0  1.0  0.0\n",
       "13   91992.39  135495.07  252664.93  134307.35  1.0  0.0  0.0\n",
       "14  119943.24  156547.42  256512.92  132602.65  0.0  1.0  0.0\n",
       "15  114523.61  122616.84  261776.23  129917.04  0.0  0.0  1.0\n",
       "16   78013.11  121597.55  264346.06  126992.93  1.0  0.0  0.0\n",
       "17   94657.16  145077.58  282574.31  125370.37  0.0  0.0  1.0\n",
       "18   91749.16  114175.79  294919.57  124266.90  0.0  1.0  0.0\n",
       "19   86419.70  153514.11       0.00  122776.86  0.0  0.0  1.0\n",
       "20   76253.86  113867.30  298664.47  118474.03  1.0  0.0  0.0\n",
       "21   78389.47  153773.43  299737.29  111313.02  0.0  0.0  1.0\n",
       "22   73994.56  122782.75  303319.26  110352.25  0.0  1.0  0.0\n",
       "23   67532.53  105751.03  304768.73  108733.99  0.0  1.0  0.0\n",
       "24   77044.01   99281.34  140574.81  108552.04  0.0  0.0  1.0\n",
       "25   64664.71  139553.16  137962.62  107404.34  1.0  0.0  0.0\n",
       "26   75328.87  144135.98  134050.07  105733.54  0.0  1.0  0.0\n",
       "27   72107.60  127864.55  353183.81  105008.31  0.0  0.0  1.0\n",
       "28   66051.52  182645.56  118148.20  103282.38  0.0  1.0  0.0\n",
       "29   65605.48  153032.06  107138.38  101004.64  0.0  0.0  1.0\n",
       "30   61994.48  115641.28   91131.24   99937.59  0.0  1.0  0.0\n",
       "31   61136.38  152701.92   88218.23   97483.56  0.0  0.0  1.0\n",
       "32   63408.86  129219.61   46085.25   97427.84  1.0  0.0  0.0\n",
       "33   55493.95  103057.49  214634.81   96778.92  0.0  1.0  0.0\n",
       "34   46426.07  157693.92  210797.67   96712.80  1.0  0.0  0.0\n",
       "35   46014.02   85047.44  205517.64   96479.51  0.0  0.0  1.0\n",
       "36   28663.76  127056.21  201126.82   90708.19  0.0  1.0  0.0\n",
       "37   44069.95   51283.14  197029.42   89949.14  1.0  0.0  0.0\n",
       "38   20229.59   65947.93  185265.10   81229.06  0.0  0.0  1.0\n",
       "39   38558.51   82982.09  174999.30   81005.76  1.0  0.0  0.0\n",
       "40   28754.33  118546.05  172795.67   78239.91  1.0  0.0  0.0\n",
       "41   27892.92   84710.77  164470.71   77798.83  0.0  1.0  0.0\n",
       "42   23640.93   96189.63  148001.11   71498.49  1.0  0.0  0.0\n",
       "43   15505.73  127382.30   35534.17   69758.98  0.0  0.0  1.0\n",
       "44   22177.74  154806.14   28334.72   65200.33  1.0  0.0  0.0\n",
       "45    1000.23  124153.04    1903.93   64926.08  0.0  0.0  1.0\n",
       "46    1315.46  115816.21  297114.46   49490.75  0.0  1.0  0.0\n",
       "47       0.00  135426.92       0.00   42559.73  1.0  0.0  0.0\n",
       "48     542.05   51743.15       0.00   35673.41  0.0  0.0  1.0\n",
       "49       0.00  116983.80   45173.06   14681.40  1.0  0.0  0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy vairables best way\n",
    "features=pd.DataFrame(features)\n",
    "features=pd.get_dummies(dataset,columns=['State'])\n",
    "features=features.iloc[:,:].values\n",
    "features=pd.DataFrame(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.2</td>\n",
       "      <td>136897.8</td>\n",
       "      <td>471784.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.7</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131876.9</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86419.7</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.3</td>\n",
       "      <td>298664.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72107.6</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.3</td>\n",
       "      <td>35534.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>116983.8</td>\n",
       "      <td>45173.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2  3\n",
       "0    165349.2   136897.8   471784.1  2\n",
       "1    162597.7  151377.59  443898.53  0\n",
       "2   153441.51  101145.55  407934.54  1\n",
       "4   142107.34   91391.77  366168.42  1\n",
       "5    131876.9   99814.71  362861.36  2\n",
       "6   134615.46  147198.87  127716.82  0\n",
       "7   130298.13  145530.06  323876.68  1\n",
       "8   120542.52  148718.95  311613.29  2\n",
       "9   123334.88  108679.17  304981.62  0\n",
       "10  101913.08  110594.11  229160.95  1\n",
       "11  100671.96   91790.61  249744.55  0\n",
       "12   93863.75  127320.38  249839.44  1\n",
       "13   91992.39  135495.07  252664.93  0\n",
       "14  119943.24  156547.42  256512.92  1\n",
       "15  114523.61  122616.84  261776.23  2\n",
       "16   78013.11  121597.55  264346.06  0\n",
       "17   94657.16  145077.58  282574.31  2\n",
       "18   91749.16  114175.79  294919.57  1\n",
       "19    86419.7  153514.11        0.0  2\n",
       "20   76253.86   113867.3  298664.47  0\n",
       "21   78389.47  153773.43  299737.29  2\n",
       "22   73994.56  122782.75  303319.26  1\n",
       "23   67532.53  105751.03  304768.73  1\n",
       "24   77044.01   99281.34  140574.81  2\n",
       "25   64664.71  139553.16  137962.62  0\n",
       "26   75328.87  144135.98  134050.07  1\n",
       "27    72107.6  127864.55  353183.81  2\n",
       "28   66051.52  182645.56   118148.2  1\n",
       "29   65605.48  153032.06  107138.38  2\n",
       "30   61994.48  115641.28   91131.24  1\n",
       "31   61136.38  152701.92   88218.23  2\n",
       "32   63408.86  129219.61   46085.25  0\n",
       "33   55493.95  103057.49  214634.81  1\n",
       "34   46426.07  157693.92  210797.67  0\n",
       "35   46014.02   85047.44  205517.64  2\n",
       "36   28663.76  127056.21  201126.82  1\n",
       "37   44069.95   51283.14  197029.42  0\n",
       "38   20229.59   65947.93   185265.1  2\n",
       "39   38558.51   82982.09   174999.3  0\n",
       "40   28754.33  118546.05  172795.67  0\n",
       "41   27892.92   84710.77  164470.71  1\n",
       "42   23640.93   96189.63  148001.11  0\n",
       "43   15505.73   127382.3   35534.17  2\n",
       "44   22177.74  154806.14   28334.72  0\n",
       "45    1000.23  124153.04    1903.93  2\n",
       "46    1315.46  115816.21  297114.46  1\n",
       "47        0.0  135426.92        0.0  0\n",
       "48     542.05   51743.15        0.0  2\n",
       "49        0.0   116983.8   45173.06  0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is second of onehotencdoing\n",
    "model= OneHotEncoder(sparse=False)\n",
    "# re_data=features[:,3].reshape(len(features[:,3]), 1)\n",
    "# data=model.fit_transform(re_data)\n",
    "# df= df.drop(['Country'], axis=1)\n",
    "data=pd.DataFrame(data)\n",
    "features=pd.DataFrame(features)\n",
    "features.join(data, rsuffix=\"_\")\n",
    "# features.join(data)\n",
    "# features=features.merge(data)\n",
    "# features.drop(3,inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables trap   (linear regression take care of the dummy variables)\n",
    "data=features.iloc[:,:-1].values\n",
    "data=pd.DataFrame(data)\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(features,label,test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.array(features)\n",
    "label=np.array(label)\n",
    "# we are doing this becuase this library dont take,x0,into the account\n",
    "import statsmodels.formula.api as sm\n",
    "features=np.append(arr=np.ones((50,1)).astype(int),values=data,axis=1)\n",
    "# print(pd.DataFrame(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinary least square model\n",
    "X_opt=features[:,[0,1,2,3,4,5,6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.947e+30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Jan 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:17:38</td>     <th>  Log-Likelihood:    </th> <td>  1093.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>  -2173.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    43</td>      <th>  BIC:               </th> <td>  -2160.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.366e-11</td> <td> 8.98e-11</td> <td>    0.486</td> <td> 0.629</td> <td>-1.37e-10</td> <td> 2.25e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-8.327e-16</td> <td> 1.14e-15</td> <td>   -0.732</td> <td> 0.468</td> <td>-3.13e-15</td> <td> 1.46e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 2.776e-16</td> <td> 4.58e-16</td> <td>    0.606</td> <td> 0.548</td> <td>-6.47e-16</td> <td>  1.2e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> -2.22e-16</td> <td> 1.54e-16</td> <td>   -1.441</td> <td> 0.157</td> <td>-5.33e-16</td> <td> 8.87e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    1.0000</td> <td> 1.32e-15</td> <td> 7.58e+14</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-2.547e-11</td> <td> 2.85e-11</td> <td>   -0.894</td> <td> 0.376</td> <td>-8.29e-11</td> <td>  3.2e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>-1.819e-11</td> <td> 2.92e-11</td> <td>   -0.623</td> <td> 0.537</td> <td>-7.71e-11</td> <td> 4.07e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.368</td> <th>  Durbin-Watson:     </th> <td>   0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.832</td> <th>  Jarque-Bera (JB):  </th> <td>   0.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.142</td> <th>  Prob(JB):          </th> <td>   0.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.581</td> <th>  Cond. No.          </th> <td>2.32e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.32e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 1.947e+30\n",
       "Date:                Sun, 16 Jan 2022   Prob (F-statistic):               0.00\n",
       "Time:                        23:17:38   Log-Likelihood:                 1093.7\n",
       "No. Observations:                  50   AIC:                            -2173.\n",
       "Df Residuals:                      43   BIC:                            -2160.\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.366e-11   8.98e-11      0.486      0.629   -1.37e-10    2.25e-10\n",
       "x1         -8.327e-16   1.14e-15     -0.732      0.468   -3.13e-15    1.46e-15\n",
       "x2          2.776e-16   4.58e-16      0.606      0.548   -6.47e-16     1.2e-15\n",
       "x3          -2.22e-16   1.54e-16     -1.441      0.157   -5.33e-16    8.87e-17\n",
       "x4             1.0000   1.32e-15   7.58e+14      0.000       1.000       1.000\n",
       "x5         -2.547e-11   2.85e-11     -0.894      0.376   -8.29e-11     3.2e-11\n",
       "x6         -1.819e-11   2.92e-11     -0.623      0.537   -7.71e-11    4.07e-11\n",
       "==============================================================================\n",
       "Omnibus:                        0.368   Durbin-Watson:                   0.244\n",
       "Prob(Omnibus):                  0.832   Jarque-Bera (JB):                0.535\n",
       "Skew:                           0.142   Prob(JB):                        0.765\n",
       "Kurtosis:                       2.581   Cond. No.                     2.32e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.32e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as smf  \n",
    "# Regressor_OLS=sm.ols(endog=label,exog=X_opt).fit()\n",
    "Regressor_OLS=smf.OLS(endog=label,exog=X_opt).fit()\n",
    "Regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>2.484e+32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Jan 2022</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:19:39</td>     <th>  Log-Likelihood:    </th>          <td>  1159.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th>          <td>  -2308.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th>          <td>  -2296.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>-4.441e-16</td> <td> 2.01e-16</td> <td>   -2.206</td> <td> 0.033</td> <td> -8.5e-16</td> <td>-3.83e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td> 5.551e-17</td> <td> 9.21e-17</td> <td>    0.603</td> <td> 0.550</td> <td> -1.3e-16</td> <td> 2.41e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td> 1.665e-16</td> <td> 4.04e-17</td> <td>    4.117</td> <td> 0.000</td> <td>  8.5e-17</td> <td> 2.48e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>    1.0000</td> <td> 2.36e-16</td> <td> 4.24e+15</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>-1.091e-11</td> <td>  7.4e-12</td> <td>   -1.474</td> <td> 0.147</td> <td>-2.58e-11</td> <td>    4e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td> 1.091e-11</td> <td> 7.67e-12</td> <td>    1.423</td> <td> 0.162</td> <td>-4.55e-12</td> <td> 2.64e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>11.225</td> <th>  Durbin-Watson:     </th> <td>   1.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.004</td> <th>  Jarque-Bera (JB):  </th> <td>  12.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.863</td> <th>  Prob(JB):          </th> <td> 0.00209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.717</td> <th>  Cond. No.          </th> <td>8.96e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 8.96e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   1.000\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              1.000\n",
       "Method:                 Least Squares   F-statistic:                          2.484e+32\n",
       "Date:                Sun, 16 Jan 2022   Prob (F-statistic):                        0.00\n",
       "Time:                        23:19:39   Log-Likelihood:                          1159.8\n",
       "No. Observations:                  50   AIC:                                     -2308.\n",
       "Df Residuals:                      44   BIC:                                     -2296.\n",
       "Df Model:                           6                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1         -4.441e-16   2.01e-16     -2.206      0.033    -8.5e-16   -3.83e-17\n",
       "x2          5.551e-17   9.21e-17      0.603      0.550    -1.3e-16    2.41e-16\n",
       "x3          1.665e-16   4.04e-17      4.117      0.000     8.5e-17    2.48e-16\n",
       "x4             1.0000   2.36e-16   4.24e+15      0.000       1.000       1.000\n",
       "x5         -1.091e-11    7.4e-12     -1.474      0.147   -2.58e-11       4e-12\n",
       "x6          1.091e-11   7.67e-12      1.423      0.162   -4.55e-12    2.64e-11\n",
       "==============================================================================\n",
       "Omnibus:                       11.225   Durbin-Watson:                   1.604\n",
       "Prob(Omnibus):                  0.004   Jarque-Bera (JB):               12.345\n",
       "Skew:                          -0.863   Prob(JB):                      0.00209\n",
       "Kurtosis:                       4.717   Cond. No.                     8.96e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 8.96e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=features[:,[1,2,3,4,5,6]]\n",
    "Regressor_OLS=smf.OLS(endog=label,exog=X_opt).fit()\n",
    "Regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>1.711e+31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Jan 2022</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:20:44</td>     <th>  Log-Likelihood:    </th>          <td>  1087.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th>          <td>  -2165.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th>          <td>  -2156.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>-5.135e-16</td> <td> 3.26e-16</td> <td>   -1.574</td> <td> 0.122</td> <td>-1.17e-15</td> <td> 1.44e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>-4.163e-17</td> <td> 1.69e-16</td> <td>   -0.247</td> <td> 0.806</td> <td>-3.81e-16</td> <td> 2.98e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    1.0000</td> <td>  5.1e-16</td> <td> 1.96e+15</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>-2.001e-11</td> <td> 3.01e-11</td> <td>   -0.666</td> <td> 0.509</td> <td>-8.05e-11</td> <td> 4.05e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>-1.819e-11</td> <td> 3.15e-11</td> <td>   -0.577</td> <td> 0.567</td> <td>-8.17e-11</td> <td> 4.53e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.517</td> <th>  Durbin-Watson:     </th> <td>   0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.284</td> <th>  Jarque-Bera (JB):  </th> <td>   1.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.391</td> <th>  Prob(JB):          </th> <td>   0.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.404</td> <th>  Cond. No.          </th> <td>8.38e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 8.38e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   1.000\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              1.000\n",
       "Method:                 Least Squares   F-statistic:                          1.711e+31\n",
       "Date:                Sun, 16 Jan 2022   Prob (F-statistic):                        0.00\n",
       "Time:                        23:20:44   Log-Likelihood:                          1087.7\n",
       "No. Observations:                  50   AIC:                                     -2165.\n",
       "Df Residuals:                      45   BIC:                                     -2156.\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1         -5.135e-16   3.26e-16     -1.574      0.122   -1.17e-15    1.44e-16\n",
       "x2         -4.163e-17   1.69e-16     -0.247      0.806   -3.81e-16    2.98e-16\n",
       "x3             1.0000    5.1e-16   1.96e+15      0.000       1.000       1.000\n",
       "x4         -2.001e-11   3.01e-11     -0.666      0.509   -8.05e-11    4.05e-11\n",
       "x5         -1.819e-11   3.15e-11     -0.577      0.567   -8.17e-11    4.53e-11\n",
       "==============================================================================\n",
       "Omnibus:                        2.517   Durbin-Watson:                   0.085\n",
       "Prob(Omnibus):                  0.284   Jarque-Bera (JB):                1.613\n",
       "Skew:                          -0.391   Prob(JB):                        0.446\n",
       "Kurtosis:                       3.404   Cond. No.                     8.38e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 8.38e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=features[:,[2,3,4,5,6]]\n",
    "Regressor_OLS=smf.OLS(endog=label,exog=X_opt).fit()\n",
    "Regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>4.622e+32</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Jan 2022</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:22:15</td>     <th>  Log-Likelihood:    </th>          <td>  1164.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th>          <td>  -2320.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th>          <td>  -2312.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td> 2.082e-16</td> <td> 3.44e-17</td> <td>    6.047</td> <td> 0.000</td> <td> 1.39e-16</td> <td> 2.77e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    1.0000</td> <td>  7.5e-17</td> <td> 1.33e+16</td> <td> 0.000</td> <td>    1.000</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>-7.276e-12</td> <td> 6.06e-12</td> <td>   -1.201</td> <td> 0.236</td> <td>-1.95e-11</td> <td> 4.91e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>-7.276e-12</td> <td> 6.52e-12</td> <td>   -1.116</td> <td> 0.270</td> <td>-2.04e-11</td> <td> 5.84e-12</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.116</td> <th>  Durbin-Watson:     </th> <td>   1.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.347</td> <th>  Jarque-Bera (JB):  </th> <td>   1.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.275</td> <th>  Prob(JB):          </th> <td>   0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.534</td> <th>  Cond. No.          </th> <td>7.22e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 7.22e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   1.000\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              1.000\n",
       "Method:                 Least Squares   F-statistic:                          4.622e+32\n",
       "Date:                Sun, 16 Jan 2022   Prob (F-statistic):                        0.00\n",
       "Time:                        23:22:15   Log-Likelihood:                          1164.0\n",
       "No. Observations:                  50   AIC:                                     -2320.\n",
       "Df Residuals:                      46   BIC:                                     -2312.\n",
       "Df Model:                           4                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1          2.082e-16   3.44e-17      6.047      0.000    1.39e-16    2.77e-16\n",
       "x2             1.0000    7.5e-17   1.33e+16      0.000       1.000       1.000\n",
       "x3         -7.276e-12   6.06e-12     -1.201      0.236   -1.95e-11    4.91e-12\n",
       "x4         -7.276e-12   6.52e-12     -1.116      0.270   -2.04e-11    5.84e-12\n",
       "==============================================================================\n",
       "Omnibus:                        2.116   Durbin-Watson:                   1.488\n",
       "Prob(Omnibus):                  0.347   Jarque-Bera (JB):                1.223\n",
       "Skew:                           0.275   Prob(JB):                        0.542\n",
       "Kurtosis:                       3.534   Cond. No.                     7.22e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 7.22e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt=features[:,[3,4,5,6]]\n",
    "Regressor_OLS=smf.OLS(endog=label,exog=X_opt).fit()\n",
    "Regressor_OLS.summary()\n",
    "\n",
    "# now train model on these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the score  \n",
    "# print('Train Score: ', regressor.score(x_train, y_train))  \n",
    "# print('Test Score: ', regressor.score(x_test, y_test))  \n",
    "# checking missing values in the data\n",
    "# data.isnull().sum()\n",
    "\n",
    "# creating the training data\n",
    "# X = data.drop(['ID', 'count'], axis=1)\n",
    "# y = data['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foward Selection  \n",
    "library for it called !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (50.3.1.post20201107)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.24.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (3.3.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->mlxtend) (1.15.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the linear regression model\n",
    "lreg = LinearRegression()\n",
    "sfs1 = sfs(lreg, k_features=2, forward=True, verbose=2, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring the library\n",
    "In the Feature Selector Model let me quickly recap what these different parameters are. The first parameter is the model name, lreg, which is basically our linear regression model.\n",
    "\n",
    "k_features tells us how many features should be selected. We’ve passed 4 so the model will train until 4 features are selected.\n",
    "\n",
    "Now here’s the difference between implementing the Backward Elimination Method and the Forward Feature Selection method, the parameter forward will be set to True. This means training the forward feature selection model. We set it as False during the backward feature elimination technique.\n",
    "\n",
    "Next, verbose = 2 will allow us to bring the model summary at each iteration.\n",
    "\n",
    "And finally, since it is a regression model scoring based on the mean squared error metric, we will set scoring = ‘neg_mean_squared_error’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0          1          2          3    4\n",
      "0   165349.20  136897.80  471784.10  192261.83  0.0\n",
      "1   162597.70  151377.59  443898.53  191792.06  1.0\n",
      "2   153441.51  101145.55  407934.54  191050.39  0.0\n",
      "3   144372.41  118671.85  383199.62  182901.99  0.0\n",
      "4   142107.34   91391.77  366168.42  166187.94  0.0\n",
      "5   131876.90   99814.71  362861.36  156991.12  0.0\n",
      "6   134615.46  147198.87  127716.82  156122.51  1.0\n",
      "7   130298.13  145530.06  323876.68  155752.60  0.0\n",
      "8   120542.52  148718.95  311613.29  152211.77  0.0\n",
      "9   123334.88  108679.17  304981.62  149759.96  1.0\n",
      "10  101913.08  110594.11  229160.95  146121.95  0.0\n",
      "11  100671.96   91790.61  249744.55  144259.40  1.0\n",
      "12   93863.75  127320.38  249839.44  141585.52  0.0\n",
      "13   91992.39  135495.07  252664.93  134307.35  1.0\n",
      "14  119943.24  156547.42  256512.92  132602.65  0.0\n",
      "15  114523.61  122616.84  261776.23  129917.04  0.0\n",
      "16   78013.11  121597.55  264346.06  126992.93  1.0\n",
      "17   94657.16  145077.58  282574.31  125370.37  0.0\n",
      "18   91749.16  114175.79  294919.57  124266.90  0.0\n",
      "19   86419.70  153514.11       0.00  122776.86  0.0\n",
      "20   76253.86  113867.30  298664.47  118474.03  1.0\n",
      "21   78389.47  153773.43  299737.29  111313.02  0.0\n",
      "22   73994.56  122782.75  303319.26  110352.25  0.0\n",
      "23   67532.53  105751.03  304768.73  108733.99  0.0\n",
      "24   77044.01   99281.34  140574.81  108552.04  0.0\n",
      "25   64664.71  139553.16  137962.62  107404.34  1.0\n",
      "26   75328.87  144135.98  134050.07  105733.54  0.0\n",
      "27   72107.60  127864.55  353183.81  105008.31  0.0\n",
      "28   66051.52  182645.56  118148.20  103282.38  0.0\n",
      "29   65605.48  153032.06  107138.38  101004.64  0.0\n",
      "30   61994.48  115641.28   91131.24   99937.59  0.0\n",
      "31   61136.38  152701.92   88218.23   97483.56  0.0\n",
      "32   63408.86  129219.61   46085.25   97427.84  1.0\n",
      "33   55493.95  103057.49  214634.81   96778.92  0.0\n",
      "34   46426.07  157693.92  210797.67   96712.80  1.0\n",
      "35   46014.02   85047.44  205517.64   96479.51  0.0\n",
      "36   28663.76  127056.21  201126.82   90708.19  0.0\n",
      "37   44069.95   51283.14  197029.42   89949.14  1.0\n",
      "38   20229.59   65947.93  185265.10   81229.06  0.0\n",
      "39   38558.51   82982.09  174999.30   81005.76  1.0\n",
      "40   28754.33  118546.05  172795.67   78239.91  1.0\n",
      "41   27892.92   84710.77  164470.71   77798.83  0.0\n",
      "42   23640.93   96189.63  148001.11   71498.49  1.0\n",
      "43   15505.73  127382.30   35534.17   69758.98  0.0\n",
      "44   22177.74  154806.14   28334.72   65200.33  1.0\n",
      "45    1000.23  124153.04    1903.93   64926.08  0.0\n",
      "46    1315.46  115816.21  297114.46   49490.75  0.0\n",
      "47       0.00  135426.92       0.00   42559.73  1.0\n",
      "48     542.05   51743.15       0.00   35673.41  0.0\n",
      "49       0.00  116983.80   45173.06   14681.40  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "\n",
      "[2022-01-16 23:39:30] Features: 1/2 -- score: -1.6755370487874129e-22[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "\n",
      "[2022-01-16 23:39:30] Features: 2/2 -- score: -7.252719610864946e-22"
     ]
    }
   ],
   "source": [
    "sfs1.fit(foward_selection,label)\n",
    "print(pd.DataFrame(foward_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3']\n"
     ]
    }
   ],
   "source": [
    "feat_names = list(sfs1.k_feature_names_)\n",
    "print(feat_names)\n",
    "# feat_names=np.array(int(feat_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new dataframe using the above variables and adding the target variable\n",
    "\n",
    "# new_data = foward_selection[feat_names]\n",
    "# new_data['count'] = data['count']\n",
    "\n",
    "# first five rows of the new data\n",
    "# new_data.head()\n",
    "\n",
    "# new grab new features and make predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
